{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93bcbe1-b35d-410f-8ecf-8ef945887268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.cnnindonesia.com/api/v2/search?query=jokowi&start=0&limit=460'\n",
    "\n",
    "response = requests.get(url)\n",
    "json_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1816760f-5ee5-4575-8581-e743c91259e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data['data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c88f7cc-9070-4bc5-9f6a-45ed2f412320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 0\n",
      "Number 1\n",
      "Number 2\n",
      "Error occur\n",
      "Number 4\n",
      "Number 5\n",
      "Number 6\n",
      "Number 7\n",
      "Number 8\n",
      "Number 9\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Playwright\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from queue import Queue\n",
    "import time\n",
    "import pandas\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "# Global variabel\n",
    "url = 'https://www.cnnindonesia.com/search/?query=jokowi&page=1'\n",
    "total_extracted_articles_in_current_page = 0\n",
    "total_extracted_pages = 0\n",
    "\n",
    "async def extract_article_data(page):\n",
    "    pass\n",
    "\n",
    "def store_extracted_article_data_to_csv(article_data):\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            if i == 3:\n",
    "                raise Exception(\"Error occur\")\n",
    "            print(f\"Number {i}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42c30098-4434-4f27-b434-7db47266c7a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "Page.goto: Timeout 30000ms exceeded.\nCall log:\n  - navigating to \"https://www.cnnindonesia.com/search/?query=jokowi&page=1\", waiting until \"load\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 170\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "Cell \u001b[0;32mIn[13], line 152\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mfirefox\u001b[38;5;241m.\u001b[39mlaunch(slow_mo\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    151\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mnew_page()\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mgoto(url)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m extract_articles_data_until_page_100(page)\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/async_api/_generated.py:8973\u001b[0m, in \u001b[0;36mPage.goto\u001b[0;34m(self, url, timeout, wait_until, referer)\u001b[0m\n\u001b[1;32m   8912\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m   8913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8914\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8920\u001b[0m     referer: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   8921\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   8922\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Page.goto\u001b[39;00m\n\u001b[1;32m   8923\u001b[0m \n\u001b[1;32m   8924\u001b[0m \u001b[38;5;124;03m    Returns the main resource response. In case of multiple redirects, the navigation will resolve with the first\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8969\u001b[0m \u001b[38;5;124;03m    Union[Response, None]\u001b[39;00m\n\u001b[1;32m   8970\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   8972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_impl_nullable(\n\u001b[0;32m-> 8973\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mgoto(\n\u001b[1;32m   8974\u001b[0m             url\u001b[38;5;241m=\u001b[39murl, timeout\u001b[38;5;241m=\u001b[39mtimeout, waitUntil\u001b[38;5;241m=\u001b[39mwait_until, referer\u001b[38;5;241m=\u001b[39mreferer\n\u001b[1;32m   8975\u001b[0m         )\n\u001b[1;32m   8976\u001b[0m     )\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_page.py:551\u001b[0m, in \u001b[0;36mPage.goto\u001b[0;34m(self, url, timeout, waitUntil, referer)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     referer: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    550\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Response]:\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main_frame\u001b[38;5;241m.\u001b[39mgoto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_frame.py:145\u001b[0m, in \u001b[0;36mFrame.goto\u001b[0;34m(self, url, timeout, waitUntil, referer)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     referer: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Response]:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    143\u001b[0m         Optional[Response],\n\u001b[1;32m    144\u001b[0m         from_nullable_channel(\n\u001b[0;32m--> 145\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoto\u001b[39m\u001b[38;5;124m\"\u001b[39m, locals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m    146\u001b[0m         ),\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_connection.py:61\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_internal_type,\n\u001b[1;32m     64\u001b[0m     )\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_connection.py:528\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_zone\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Page.goto: Timeout 30000ms exceeded.\nCall log:\n  - navigating to \"https://www.cnnindonesia.com/search/?query=jokowi&page=1\", waiting until \"load\"\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Playwright\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from queue import Queue\n",
    "import time\n",
    "import pandas\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "# Global variabel\n",
    "url = 'https://www.cnnindonesia.com/search/?query=jokowi&page=1'\n",
    "total_extracted_articles_in_current_page = 0\n",
    "total_extracted_pages = 0\n",
    "is_initial_run = True\n",
    "\n",
    "async def scroll_to_and_click_article(article_selector):\n",
    "    await article_selector.scroll_into_view_if_needed()\n",
    "    await article_selector.click()\n",
    "\n",
    "async def get_all_article_selectors_in_current_page(page):\n",
    "    article_selector_queues = Queue()\n",
    "    \n",
    "    try:\n",
    "        # data-name=\"cnn-id\"\n",
    "        article_selectors = await page.locator('xpath=//div[contains(@data-name, \"cnn-id\")]/div/article').all()\n",
    "\n",
    "        for article_selector in article_selectors:\n",
    "            article_selector_queues.put(article_selector)\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred while get all article selectors in page {total_extracted_pages + 1}: {e}')\n",
    "    \n",
    "    return article_selector_queues\n",
    "\n",
    "async def extract_article_data(page):\n",
    "    global total_extracted_articles_in_current_page\n",
    "    \n",
    "    article_data = {\n",
    "        'article_title' : '',\n",
    "        'article_author': '',\n",
    "        'article_publication_date' : '',\n",
    "        'article_content' : '',\n",
    "    }\n",
    "    \n",
    "    try:        \n",
    "        article_title_selector = page.locator('xpath=//h1[contains(@class, \"leading-9\")]').first\n",
    "        article_publication_date_selector = page.locator('xpath=//div[contains(@class, \"w-leftcontent\")]/div[4]').first \n",
    "        article_content_selectors = await page.locator('xpath=//div[contains(@class, \"detail-text\")]/p').all()\n",
    "    \n",
    "        # Joining all article content\n",
    "        article_content = ''\n",
    "        for article_content_selector in article_content_selectors:\n",
    "            ac = await article_content_selector.inner_text()\n",
    "            article_content += '\\n' + ac.replace('\\n', '').strip()\n",
    "        \n",
    "        article_data['article_title'] = (await article_title_selector.inner_text()).strip()\n",
    "        article_data['article_author'] = 'CNN Indonesia'\n",
    "        article_data['article_publication_date'] = (await article_publication_date_selector.inner_text()).strip()\n",
    "        # Remove leading and trailing newline characters from the article content\n",
    "        article_data['article_content'] = article_content.strip()\n",
    "\n",
    "        total_extracted_articles_in_current_page += 1\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred while get article data at page {total_extracted_pages + 1} in article number {total_extracted_articles_in_current_page + 1}: {e}')\n",
    "    \n",
    "    # Wait for a second and then go back to previous page\n",
    "    wait_time_before_go_to_previous_page = random.randint(5, 10) \n",
    "    print(f'Waiting for {wait_time_before_go_to_previous_page} seconds before go to previous page')\n",
    "    time.sleep(wait_time_before_go_to_previous_page)\n",
    "    await page.go_back()\n",
    "\n",
    "    return article_data\n",
    "\n",
    "def store_extracted_article_data_to_csv(article_data):\n",
    "    dataframe = pandas.DataFrame([article_data])\n",
    "\n",
    "    try:\n",
    "        if os.path.isfile('./cnnindonesia_unprocessed_news_data.csv') == True:\n",
    "            dataframe.to_csv('cnnindonesia_unprocessed_news_data.csv', sep='\\t', encoding='utf-8', index=False, header=False, mode='a')\n",
    "        else:\n",
    "            dataframe.to_csv('cnnindonesia_unprocessed_news_data.csv', sep='\\t', encoding='utf-8', index=False, header=True)\n",
    "    except Exception as e:\n",
    "        print(f'Error occured while saved article data to csv at page {total_extracted_pages + 1} in article number {total_extracted_articles_in_current_page + 1}: {e}')\n",
    "\n",
    "async def extract_articles_data_until_page_100(page):\n",
    "    global total_extracted_pages, total_extracted_articles_in_current_page, is_initial_run\n",
    "    \n",
    "    article_selector_queues = Queue()\n",
    "\n",
    "    # Scraping until 100 pages\n",
    "    while total_extracted_pages <= 100:\n",
    "        try:\n",
    "            print(f'Awaiting until page number {total_extracted_pages + 1} to load')\n",
    "            time.sleep(random.randint(10, 15))\n",
    "            await page.wait_for_load_state('domcontentloaded')\n",
    "            print(f'Page number {total_extracted_pages + 1} has been loaded')\n",
    "\n",
    "            if article_selector_queues.empty() == True and is_initial_run == True:\n",
    "                article_selector_queues = await get_all_article_selectors_in_current_page(page)\n",
    "                is_initial_run = False\n",
    "            elif article_selector_queues.empty() == True and is_initial_run != True:\n",
    "                article_selector_queues = await get_all_article_selectors_in_current_page(page)    \n",
    "                total_extracted_articles_in_current_page = 0\n",
    "                total_extracted_pages += 1\n",
    "            \n",
    "            if article_selector_queues.empty() != True and is_initial_run != True:\n",
    "                print(f'Scroll to and click article number {total_extracted_articles_in_current_page + 1} at page {total_extracted_pages + 1}')\n",
    "                await scroll_to_and_click_article(article_selector_queues.get())\n",
    "                print(f'Waiting page to load after clicked article number {total_extracted_articles_in_current_page + 1} at page {total_extracted_pages + 1}')\n",
    "                time.sleep(random.randint(10, 15))\n",
    "                await page.wait_for_load_state()\n",
    "                print(f'Page has been load for article number {total_extracted_articles_in_current_page + 1} at page {total_extracted_pages + 1}')\n",
    "\n",
    "                print(f'Start scraping article number {total_extracted_articles_in_current_page + 1} at page {total_extracted_pages + 1}')\n",
    "                article_data = await extract_article_data(page)\n",
    "                wait_time_after_done_scraping_article = random.randint(10, 15)\n",
    "                print(f'Done scraping article  number {total_extracted_articles_in_current_page} at page {total_extracted_pages + 1} and waiting for {wait_time_after_done_scraping_article} seconds')\n",
    "                time.sleep(wait_time_after_done_scraping_article)\n",
    "\n",
    "                print(f'Store result of scraping article number {total_extracted_articles_in_current_page} at page {total_extracted_pages + 1} into csv file')\n",
    "                store_extracted_article_data_to_csv(article_data)\n",
    "                print(f'Result stored in csv file for article number {total_extracted_articles_in_current_page} at page {total_extracted_pages + 1}\\n')\n",
    "            elif article_selector_queues.empty() == True and is_initial_run != True:\n",
    "                clear_output(wait=True)\n",
    "                wait_time_after_done_scraping_all_articles_at_page = random.randint(20, 30)\n",
    "                print(f'Done scraping all articles at page {total_extracted_pages + 1} and waiting for {wait_time_after_done_scraping_all_articles_at_page} seconds')\n",
    "                time.sleep(wait_time_after_done_scraping_all_articles_at_page)\n",
    "\n",
    "                # Get the next button page, then scroll to that button and click\n",
    "                print(f'Navigate to next page')\n",
    "                new_page_button_html_selector = page.locator(\n",
    "                        'xpath=//a[contains(@dtr-act, \"halaman selanjutnya\")]').first\n",
    "                await new_page_button_html_selector.scroll_into_view_if_needed()\n",
    "                await new_page_button_html_selector.click()\n",
    "                print('\\n\\n')\n",
    "        except Exception as e:\n",
    "            print(f'Error occur when scraping article number {total_extracted_articles_in_current_page + 1} at page {total_extracted_pages + 1}: {e}')\n",
    "            \n",
    "            # Restarting scraping\n",
    "            wait_time_before_restrart = random.randint(20, 30)\n",
    "            print(f'Restarting scraping in {wait_time_before_restrart} seconds')\n",
    "            time.sleep(wait_time_before_restrart)\n",
    "            \n",
    "            await page.go_back()\n",
    "            \n",
    "            continue\n",
    "\n",
    "async def main():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.firefox.launch(slow_mo=100)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "    \n",
    "        try:\n",
    "            await extract_articles_data_until_page_100(page)\n",
    "        except Exception as e:\n",
    "            print(f'Error occur when scraping: {e}')\n",
    "        finally:\n",
    "            dataframe_scraping_summary = pandas.DataFrame([{\n",
    "                'total_extracted_articles_in_current_page' : total_extracted_articles_in_current_page,\n",
    "                'total_extracted_pages': total_extracted_pages,\n",
    "            }])\n",
    "    \n",
    "            dataframe_scraping_summary.to_json('cnnindonesia_scraping_summary.json', orient='records', lines=True)\n",
    "\n",
    "            print(f'Scraping stopped and please see scraping_summary.json file')\n",
    "            await browser.close()\n",
    "               \n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10734d92-b05c-4263-9b4e-9e308b20f904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-venv",
   "language": "python",
   "name": "python-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
