{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39ffb75-b6f7-42b1-b4d7-09269da4c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scraping all articles at page 98\n",
      "Waiting for 23 seconds\n",
      "Scraping article number 1 at page 99\n",
      "Done scraping article and waiting for 15 seconds\n",
      "Scraping article number 2 at page 99\n",
      "Done scraping article and waiting for 15 seconds\n",
      "Scraping article number 3 at page 99\n",
      "Error occur when scraping and please see scraping_summary.json file: Page.go_back: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - waiting for navigation until \"load\"\n",
      "\n",
      "Restarting scraping in 24 seconds\n",
      "Scraping article number 1 at page 100\n",
      "Done scraping article and waiting for 13 seconds\n",
      "Scraping article number 2 at page 100\n",
      "Done scraping article and waiting for 12 seconds\n",
      "Scraping article number 3 at page 100\n",
      "Done scraping article and waiting for 11 seconds\n",
      "Scraping article number 4 at page 100\n",
      "Done scraping article and waiting for 15 seconds\n",
      "Scraping article number 5 at page 100\n",
      "Error occur when scraping and please see scraping_summary.json file: Page.go_back: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "  - waiting for navigation until \"load\"\n",
      "\n",
      "Restarting scraping in 20 seconds\n",
      "Scraping article number 1 at page 101\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n",
      "Scraping stopped and please see scraping_summary.json file\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m--> 109\u001b[0m article_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m extract_article_data(page)\n\u001b[1;32m    110\u001b[0m wait_time_after_done_scraping_article \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m, in \u001b[0;36mextract_article_data\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m     68\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mgo_back()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m article_data\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/async_api/_generated.py:9192\u001b[0m, in \u001b[0;36mPage.go_back\u001b[0;34m(self, timeout, wait_until)\u001b[0m\n\u001b[1;32m   9163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Page.go_back\u001b[39;00m\n\u001b[1;32m   9164\u001b[0m \n\u001b[1;32m   9165\u001b[0m \u001b[38;5;124;03mReturns the main resource response. In case of multiple redirects, the navigation will resolve with the response of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9188\u001b[0m \u001b[38;5;124;03mUnion[Response, None]\u001b[39;00m\n\u001b[1;32m   9189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   9191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_impl_nullable(\n\u001b[0;32m-> 9192\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mgo_back(timeout\u001b[38;5;241m=\u001b[39mtimeout, waitUntil\u001b[38;5;241m=\u001b[39mwait_until)\n\u001b[1;32m   9193\u001b[0m )\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_page.py:590\u001b[0m, in \u001b[0;36mPage.go_back\u001b[0;34m(self, timeout, waitUntil)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgo_back\u001b[39m(\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    586\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    587\u001b[0m     waitUntil: DocumentLoadState \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    588\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Response]:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_nullable_channel(\n\u001b[0;32m--> 590\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoBack\u001b[39m\u001b[38;5;124m\"\u001b[39m, locals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m    591\u001b[0m     )\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_connection.py:61\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_internal_type,\n\u001b[1;32m     64\u001b[0m     )\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_connection.py:528\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Page.go_back: Timeout 30000ms exceeded.\nCall log:\n  - waiting for navigation until \"load\"\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 167\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     total_extracted_pages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.detik.com/search/searchnews?query=jokowi&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_extracted_pages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&result_type=relevansi&fromdatex=20/10/2014&todatex=20/10/2024\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     dataframe_scraping_summary \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_extracted_articles_in_current_page\u001b[39m\u001b[38;5;124m'\u001b[39m : total_extracted_articles_in_current_page,\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_extracted_pages\u001b[39m\u001b[38;5;124m'\u001b[39m: total_extracted_pages,\n\u001b[1;32m    159\u001b[0m     }])\n",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     total_extracted_pages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.detik.com/search/searchnews?query=jokowi&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_extracted_pages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&result_type=relevansi&fromdatex=20/10/2014&todatex=20/10/2024\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     dataframe_scraping_summary \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_extracted_articles_in_current_page\u001b[39m\u001b[38;5;124m'\u001b[39m : total_extracted_articles_in_current_page,\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_extracted_pages\u001b[39m\u001b[38;5;124m'\u001b[39m: total_extracted_pages,\n\u001b[1;32m    159\u001b[0m     }])\n",
      "    \u001b[0;31m[... skipping similar frames: main at line 154 (37 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     total_extracted_pages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.detik.com/search/searchnews?query=jokowi&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_extracted_pages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&result_type=relevansi&fromdatex=20/10/2014&todatex=20/10/2024\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     dataframe_scraping_summary \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_extracted_articles_in_current_page\u001b[39m\u001b[38;5;124m'\u001b[39m : total_extracted_articles_in_current_page,\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_extracted_pages\u001b[39m\u001b[38;5;124m'\u001b[39m: total_extracted_pages,\n\u001b[1;32m    159\u001b[0m     }])\n",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scroll_to_and_click_article(article_selector_queues\u001b[38;5;241m.\u001b[39mget())\n\u001b[1;32m    107\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m--> 109\u001b[0m article_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m extract_article_data(page)\n\u001b[1;32m    110\u001b[0m wait_time_after_done_scraping_article \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone scraping article and waiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time_after_done_scraping_article\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m, in \u001b[0;36mextract_article_data\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m     36\u001b[0m article_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_title\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_author\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_publication_date\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_content\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mwait_for_load_state()\n\u001b[1;32m     46\u001b[0m     article_title_selector \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mlocator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpath=//h1[contains(@class, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail__title\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfirst\n\u001b[1;32m     47\u001b[0m     article_author_selector \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mlocator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpath=//div[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail__author\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]|//div[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]/div[2]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfirst\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/async_api/_generated.py:9069\u001b[0m, in \u001b[0;36mPage.wait_for_load_state\u001b[0;34m(self, state, timeout)\u001b[0m\n\u001b[1;32m   9017\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_load_state\u001b[39m(\n\u001b[1;32m   9018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9019\u001b[0m     state: typing\u001b[38;5;241m.\u001b[39mOptional[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9023\u001b[0m     timeout: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   9024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Page.wait_for_load_state\u001b[39;00m\n\u001b[1;32m   9026\u001b[0m \n\u001b[1;32m   9027\u001b[0m \u001b[38;5;124;03m    Returns when the required load state has been reached.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9065\u001b[0m \u001b[38;5;124;03m        `page.set_default_timeout()` methods.\u001b[39;00m\n\u001b[1;32m   9066\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   9068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_maybe_impl(\n\u001b[0;32m-> 9069\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mwait_for_load_state(state\u001b[38;5;241m=\u001b[39mstate, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   9070\u001b[0m     )\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_page.py:567\u001b[0m, in \u001b[0;36mPage.wait_for_load_state\u001b[0;34m(self, state, timeout)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_load_state\u001b[39m(\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    564\u001b[0m     state: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomcontentloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkidle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    565\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main_frame\u001b[38;5;241m.\u001b[39mwait_for_load_state(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_frame.py:243\u001b[0m, in \u001b[0;36mFrame.wait_for_load_state\u001b[0;34m(self, state, timeout)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_load_state\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     state: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomcontentloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkidle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    242\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_load_state_impl(state, timeout)\n",
      "File \u001b[0;32m~/Repositories/sentiment-analysis-jokowi-10-year/env/lib/python3.13/site-packages/playwright/_impl/_frame.py:271\u001b[0m, in \u001b[0;36mFrame._wait_for_load_state_impl\u001b[0;34m(self, state, timeout)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m actual_state \u001b[38;5;241m==\u001b[39m state\n\u001b[1;32m    266\u001b[0m     waiter\u001b[38;5;241m.\u001b[39mwait_for_event(\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_emitter,\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloadstate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    269\u001b[0m         handle_load_state_event,\n\u001b[1;32m    270\u001b[0m     )\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mresult()\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Playwright\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from queue import Queue\n",
    "import time\n",
    "import pandas\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "# Global variabel\n",
    "url = 'https://www.detik.com/search/searchnews?query=jokowi&page=39&result_type=relevansi&fromdatex=20/10/2014&todatex=20/10/2024'\n",
    "total_extracted_articles_in_current_page = 0\n",
    "total_extracted_pages = 38\n",
    "\n",
    "async def get_all_article_selectors_in_current_page(page):\n",
    "    article_selector_queues = Queue()\n",
    "    \n",
    "    try:\n",
    "        article_selectors = await page.locator('xpath=//div[contains(@class, \"list-content\")]/article[contains(@class, \"list-content__item\")]').all()\n",
    "\n",
    "        for article_selector in article_selectors:\n",
    "            article_selector_queues.put(article_selector)\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred while get all article selectors in page {total_extracted_pages + 1}: {e}')\n",
    "    \n",
    "    return article_selector_queues\n",
    "\n",
    "async def scroll_to_and_click_article(article_selector):\n",
    "    await article_selector.scroll_into_view_if_needed()\n",
    "    await article_selector.click()\n",
    "    \n",
    "async def extract_article_data(page):\n",
    "    global total_extracted_articles_in_current_page\n",
    "    \n",
    "    article_data = {\n",
    "        'article_title' : '',\n",
    "        'article_author': '',\n",
    "        'article_publication_date' : '',\n",
    "        'article_content' : '',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        await page.wait_for_load_state()\n",
    "        \n",
    "        article_title_selector = page.locator('xpath=//h1[contains(@class, \"detail__title\")]').first\n",
    "        article_author_selector = page.locator('xpath=//div[@class=\"detail__author\"]|//div[@class=\"detail\"]/div[2]').first\n",
    "        article_publication_date_selector = page.locator('xpath=//div[contains(@class, \"detail__date\")]').first \n",
    "        article_content_selectors = await page.locator('xpath=//div[contains(@class, \"detail__body-text\")]/p').all()\n",
    "    \n",
    "        # Joining all article content\n",
    "        article_content = ''\n",
    "        for article_content_selector in article_content_selectors:\n",
    "            ac = await article_content_selector.inner_text()\n",
    "            article_content += '\\n' + ac.replace('\\n', '').strip()\n",
    "        \n",
    "        article_data['article_title'] = (await article_title_selector.inner_text()).strip()\n",
    "        article_data['article_author'] = (await article_author_selector.inner_text()).strip()\n",
    "        article_data['article_publication_date'] = (await article_publication_date_selector.inner_text()).strip()\n",
    "        # Remove leading and trailing newline characters from the article content\n",
    "        article_data['article_content'] = article_content.strip()\n",
    "\n",
    "        total_extracted_articles_in_current_page += 1\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred while get article data at page {total_extracted_pages + 1} in article number {total_extracted_articles_in_current_page + 1}: {e}')\n",
    "    \n",
    "    # Sleep for a second and then go back to previous page\n",
    "    time.sleep(random.randint(5, 10))\n",
    "    await page.go_back()\n",
    "\n",
    "    return article_data\n",
    "\n",
    "def store_extracted_article_data_to_csv(article_data):\n",
    "    dataframe = pandas.DataFrame([article_data])\n",
    "\n",
    "    try:\n",
    "        if os.path.isfile('./detikcom_unprocessed_news_data.csv') == True:\n",
    "            dataframe.to_csv('detikcom_unprocessed_news_data.csv', sep='\\t', encoding='utf-8', index=False, header=False, mode='a')\n",
    "        else:\n",
    "            dataframe.to_csv('detikcom_unprocessed_news_data.csv', sep='\\t', encoding='utf-8', index=False, header=True)\n",
    "    except Exception as e:\n",
    "        print(f'Error occured while saved article data to csv at page {total_extracted_pages + 1} in article number {total_extracted_articles_in_current_page + 1}: {e}')\n",
    "    \n",
    "\n",
    "async def main():\n",
    "    global total_extracted_articles_in_current_page, total_extracted_pages, url\n",
    "    \n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            # For test purpose only\n",
    "            # browser = await p.firefox.launch(headless=False, slow_mo=100)\n",
    "            \n",
    "            browser = await p.firefox.launch(slow_mo=100)\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "            await page.wait_for_load_state()\n",
    "            time.sleep(random.randint(2, 5))\n",
    "            \n",
    "            article_selector_queues = await get_all_article_selectors_in_current_page(page)\n",
    "\n",
    "            while total_extracted_pages <= 1000:\n",
    "                print(f'Scraping article number {total_extracted_articles_in_current_page + 1} at page {total_extracted_pages + 1}')\n",
    "                await page.wait_for_load_state()\n",
    "                \n",
    "                if article_selector_queues.empty() != True:\n",
    "                    await scroll_to_and_click_article(article_selector_queues.get())\n",
    "                    time.sleep(random.randint(2, 5))\n",
    "                    \n",
    "                    article_data = await extract_article_data(page)\n",
    "                    wait_time_after_done_scraping_article = random.randint(10, 15)\n",
    "                    print(f'Done scraping article and waiting for {wait_time_after_done_scraping_article} seconds')\n",
    "                    time.sleep(wait_time_after_done_scraping_article)\n",
    "                    \n",
    "                    store_extracted_article_data_to_csv(article_data)\n",
    "                    time.sleep(random.randint(2, 5))\n",
    "                else:\n",
    "                    clear_output(wait=True)\n",
    "                    wait_time_after_done_scraping_all_articles_at_page = random.randint(20, 30)\n",
    "                    print(f'Done scraping all articles at page {total_extracted_pages + 1}\\nWaiting for {wait_time_after_done_scraping_all_articles_at_page} seconds')\n",
    "                    time.sleep(wait_time_after_done_scraping_all_articles_at_page)\n",
    "                    \n",
    "                    # Get the next button page, then scroll to that button and click\n",
    "                    new_page_button_html_selector = page.locator(\n",
    "                        'xpath=//div[contains(@class, \"pagination\")]/a[contains(@class, \"pagination__item\")]').last\n",
    "                    await new_page_button_html_selector.scroll_into_view_if_needed()\n",
    "                    await new_page_button_html_selector.click()\n",
    "                    await page.wait_for_load_state()\n",
    "\n",
    "                    article_selector_queues = await get_all_article_selectors_in_current_page(page)\n",
    "                    \n",
    "                    total_extracted_articles_in_current_page = 0\n",
    "                    total_extracted_pages += 1\n",
    "    except Exception as e:\n",
    "        print(f'Error occur when scraping and please see scraping_summary.json file: {e}')\n",
    "        await browser.close()\n",
    "        \n",
    "        dataframe_scraping_summary = pandas.DataFrame([{\n",
    "            'total_extracted_articles_in_current_page' : total_extracted_articles_in_current_page,\n",
    "            'total_extracted_pages': total_extracted_pages,\n",
    "        }])\n",
    "    \n",
    "        dataframe_scraping_summary.to_json('detikcom_scraping_summary.json', orient='records', lines=True)\n",
    "\n",
    "        total_extracted_articles_in_current_page = 0\n",
    "        \n",
    "        # Restarting scraping\n",
    "        wait_time_before_restrart = random.randint(20, 30)\n",
    "        print(f'Restarting scraping in {wait_time_before_restrart} seconds')\n",
    "        time.sleep(wait_time_before_restrart)\n",
    "        \n",
    "        # Skip article at error page and continue\n",
    "        total_extracted_pages += 1\n",
    "        url = f'https://www.detik.com/search/searchnews?query=jokowi&page={total_extracted_pages}&result_type=relevansi&fromdatex=20/10/2014&todatex=20/10/2024'\n",
    "        await main()\n",
    "    finally:\n",
    "        dataframe_scraping_summary = pandas.DataFrame([{\n",
    "            'total_extracted_articles_in_current_page' : total_extracted_articles_in_current_page,\n",
    "            'total_extracted_pages': total_extracted_pages,\n",
    "        }])\n",
    "    \n",
    "        dataframe_scraping_summary.to_json('detikcom_scraping_summary.json', orient='records', lines=True)\n",
    "\n",
    "        print(f'Scraping stopped and please see scraping_summary.json file')\n",
    "        await browser.close()\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5214b49-554e-44fb-a5ee-42799ea70dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-venv",
   "language": "python",
   "name": "python-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
